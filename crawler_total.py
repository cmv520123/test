# keyword = 'ä¿åŠ›é”'
def crawler_all(keyword):
#     from bs4 import BeautifulSoup
#     from openpyxl import Workbook
#     import parsel #æ•¸æ“šè§£ææ¨¡çµ„
#     import requests #æ•¸æ“šè«‹æ±‚æ¨¡çµ„
#     import csv
#     import json
#     from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
#     import re
#     import pandas as pd
#     from json.decoder import JSONDecodeError
#     import os
    # keywords_list = ['è€å”ç','ç”°åŸé¦™','è¾²ç´”é„‰','èŠ³èŒ²','å¨˜å®¶','ç´”ç…‰']
    # ç‚ºé˜²çˆ¬èŸ²ç›£æ§ æ‰€ä»¥æ”¹æˆè‡ªå·±çš„user-agent
    # headers = {
    #     'æ”¹æˆè‡ªå·±çš„user-agent'
    # }
    # #ç‚ºé˜²çˆ¬èŸ²ç›£æ§ æ‰€ä»¥æ”¹æˆè‡ªå·±çš„user-agent
    # raw_folder = r'C:\Users\User\Desktop\0614_datasets\ '+keyword
    # txt_folder = r'C:\Users\User\Desktop\0614_datasets\ '

    os.makedirs(keyword+'_data',exist_ok = True)
    os.makedirs(keyword+'_analysis',exist_ok = True)
    headers = {
        'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36'
    }

    ILLEGAL_CHARACTERS_RE = re.compile(r'[\000-\010]|[\013-\014]|[\016-\037]')

    wb = Workbook()
    ws = wb.active

    title =["æ–‡ç« æ¨™é¡Œ","ç™¼ä½ˆæ—¥æœŸ",'é—œéµå­—',"æ–‡ç« å…§å®¹"]
    ws.append(title)
    web = 'åª½å’ªæ‹œ'

    article_url=[]
    article_url_2 = []

    for i in range(1,20):
        try:
            url = 'https://mamibuy.com.tw/search/'+keyword+'?p='+str(i)+'&s=1'
            response = requests.get(url=url , headers = headers)
            selector = parsel.Selector(response.text) # æŠŠç²å–ä¸‹ä¾†çš„htmlå­—ç¬¦ä¸²æ•¸æ“šï¼Œè½‰æˆselectorå¯è§£æçš„å°è±¡
        # print (selector) #<Selector xpath=None data='<html lang="zh-CN" class="ua-mac ua-w...'>
        # cssé¸æ“‡å™¨ï¼šå°±æ˜¯æ ¹æ“šæ¨™ç±¤å±¬æ€§å…§å®¹ï¼Œæå–ç›¸é—œæ•¸æ“š
            lis = selector.css('.well .link-black') #ç¬¬ä¸€æ¬¡æå–ï¼Œç²å–æ‰€æœ‰liæ¨™ç±¤ è¿”å›åˆ—è¡¨
        # print(lis)
            for li in lis :
                href = li.css('a::attr(href)').get() #è©³æƒ…é  ç²å–attrå±¬æ€§
                article_url.append(href)
        except KeyError as k :
            print(k)     
        except AttributeError as attr:
            print(attr)
        except NameError as name:
            print(name)
        except IndexError as index:
            print(index)
        except JSONDecodeError :
            print(JSONDecodeError)
        # print(article_url)
        # break
    # print(len(article_url))

    for i in article_url :
        y = i.replace('https://mamibuy.com.tw','')
        z = 'https://mamibuy.com.tw'+ y
        article_url_2.append(z)
        # print(z)

    print(web + 'ç¸½å…±æœ‰'+str(len(article_url_2))+'å€‹ç¶²å€') 

    # çˆ¬å–æ‰€æœ‰æ–‡ç« çš„å…§æ–‡
    num = 0 
    for url in article_url_2:
        # try:
        # print(url)
        # 1.ç™¼é€è«‹æ±‚

        res = requests.get(url=url,headers=headers)

        soup = BeautifulSoup(res.text,"lxml")
        json_file = soup.select("script[type='application/ld+json']")
        # j = json_file[1]
        # print(json_file) 
        # print(type(json_file))
        #<class 'bs4.element.Tag'>
        try:
            for i in json_file:

                targets = json.loads(i.get_text(strip=True),strict=False)
                # strict=False ä¸è¦é‚£éº¼åš´è¬¹ å­—å…¸å–ä¸åˆ°çš„ å°±ç®—äº†
                # print(targets['articleBody'])
                titles_list=["headline","datePublished",'keywords',"articleBody"]
                # title =["æ–‡ç« æ¨™é¡Œ","ç™¼ä½ˆæ—¥æœŸ",'é—œéµå­—',"æ–‡ç« å…§å®¹"]
                article_info = []
                try :
                    for title in titles_list:                
                        y = targets[title]
                        y= ILLEGAL_CHARACTERS_RE.sub(r'', y)
                        article_info.append(y)
                        # print(title,':',y)    
                except KeyError as k :
                    print(k)             
                except AttributeError as attr:
                    print(attr)
                except NameError as name:
                    print(name)
                except IndexError as index:
                    print(index)
                except JSONDecodeError :
                    print(JSONDecodeError)
                if len(article_info) != 0 :
                    num+=1
                    ws.append(article_info)
                    print(web + 'å·²å®Œæˆå¯«å…¥' + str(num) + 'ç¯‡æ–‡ç« ')
        except KeyError as k :
            print(k)       
        except AttributeError as attr:
            print(attr)
        except NameError as name:
            print(name)
        except IndexError as index:
            print(index)
        except JSONDecodeError :
            print(JSONDecodeError)

    wb.save(keyword +'_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx')

    import pandas as pd
    df = pd.read_excel(keyword +'_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´
    print(keyword+'_'+web+ '_ç¸½å…±æœ‰'+str(len(df))+'ç¯‡æ–‡ç« ')

    #åª½å’ªæ„›çˆ¬èŸ²

    from bs4 import BeautifulSoup
    from openpyxl import Workbook
    import parsel #æ•¸æ“šè§£ææ¨¡çµ„
    import requests #æ•¸æ“šè«‹æ±‚æ¨¡çµ„
    import csv
    import json
    from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
    import openpyxl
    import re

    wb = Workbook()
    ws = wb.active

    title = ["æ–‡ç« ID","æ–‡ç« æ¨™é¡Œ","æ–‡ç« å…§å®¹","ç•™è¨€æ•¸","ç•™è¨€å…§å®¹"]

    ws.append(title)
    ILLEGAL_CHARACTERS_RE = re.compile(r'[\000-\010]|[\013-\014]|[\016-\037]')
    web = 'åª½å’ªæ„›'

    url = 'https://mamilove.com.tw/v3/questions/search?query='+str(keyword)+'&per_page=10000'

    res = requests.get(url)
    res_json = res.json()
    love_json = res_json['data']
        
    num =0 
    for article in love_json:
        try:
            article_info = []
            titles_list=["question_id","subject","content"]


            for title in titles_list:                
                y = article[title]

                article_info.append(y)
                # print(title,':',y)
                
            d = article["question_meta"]["answer_count"]
            title_2 = 'answer_count'
            article_info.append(d)
            # print(title_2,':',d)


            #     for j in love_json:
            #         article_ID = love_json["question_id"]
            article_ID = article['question_id']
            url = 'https://mamilove.com.tw/v3/question/'+str(article_ID)+'/answers?per_page=200'
            message = requests.get(url)
            message_json = message.json()
            clean_message = message_json['data']
            pure_message = ''
            for w in clean_message:
                pure_message += w['content']
                pure_message = str(pure_message)
                pure_message= ILLEGAL_CHARACTERS_RE.sub(r'', pure_message)
            #             pure_message = pure_message.replace('[\000-\010]','').replace('|[\013-\014]','').replace('|[\016-\037]','')
            article_info.append(pure_message)
            title_3 = 'message'
            # print(title_3,':',pure_message)
            num += 1
            ws.append(article_info)
            print(web + 'å·²å®Œæˆå¯«å…¥' + str(num) + 'ç¯‡æ–‡ç« ')
        except KeyError as k :
            print(k)        
        except AttributeError as attr:
            print(attr)
        except NameError as name:
            print(name)
        except IndexError as index:
            print(index)
        except JSONDecodeError :
            print(JSONDecodeError) 

    #         print(article_info)
    #         break

    wb.save(keyword +'_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx')

    df = pd.read_excel(keyword + '_'+ web +'_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´
    print(keyword+'_'+web+ '_ç¸½å…±æœ‰'+str(len(df))+'ç¯‡æ–‡ç« ')

    #çˆ¬å–Dcardæ·±å¡è²¼æ–‡æ¨™é¡ŒåŠå…§æ–‡
    import requests as req
    from openpyxl import Workbook
    import numpy as np
    from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
    import re
    import pandas as pd

    #æ–°å¢ExcelåŠå·¥ä½œè¡¨
    wb = Workbook()
    ws = wb.active
    ILLEGAL_CHARACTERS_RE = re.compile(r'[\000-\010]|[\013-\014]|[\016-\037]')
    #æ–°å¢title
    title = ['ç™¼ä½ˆæ—¥æœŸ','æ–‡ç« æ¨™é¡Œ','æ–‡ç« å…§å®¹','ç•™è¨€æ•¸','å–œæ­¡æ•¸','æ¨™ç±¤']
    ws.append(title)
    web = 'Dcardæ·±å¡'

    #çˆ¬å–çš„æ—¥æœŸ 2019/1/1~2022/12/28
    years = ['2019','2020','2021','2022']
    # dates = ['01','15','28']
    dates = ['01','28']

    num = 0
    for year in years:    
        for month in range(1,13,3):
            for date in dates:
                try:

                    # print(year+'-'+str(month)+'-'+date)
                    #ä¸æ–·æ›´æ”¹ç¶²å€ ä¸æ–·å‰å¾€ç¶²ç«™çˆ¬å–
                    url = 'https://tw.observer/api/posts?hot=0&before='+year+'-'+str(month)+'-'+date+'&term='+keyword
                    #https://tw.observer/api/posts?hot=0&before=2022-05-09&term=%æ»´é›ç²¾
                    r = req.get(url,headers=headers)
                    #ä»¥jsonæ ¼å¼è®€å–
                    rj = r.json()
                    #æ‹†è§£è³‡æ–™æ ¼å¼-å­—å…¸
                    #{'data': {'posts': 
                    # [{'id': 230313090, 'title': 'ç›®å‰æ‡·å­•16é€±çš„åª½åª½', 
                    # 'content': 'æ‡·å­•è‡³ç›®å‰16é€±äº†\næ‡·å­•ä¹‹å‰æ˜¯166/59ï¼Œçµæœå‰ä¸‰å€‹æœˆéå¸¸åš´é‡çš„å­•åè®“é«”é‡ç›´ç›´è½ç˜¦åˆ°51é‚„ä¼´éš¨è‘—äº›è¨±è½ç´…\nå¥½ä¸å®¹æ˜“ç†¬éä½†é£Ÿæ…¾é‚„æ˜¯ä¸è¦‹èµ·è‰²\nç›®å‰é£Ÿé‡å¤§ç´„5é¡†æ°´é¤ƒä¸€é¤ï¼Œå°±ç®—æ²’åƒä¹Ÿä¸è¦ºå¾—é¤“ï¼Œè¦ºå¾—å°ä¸èµ·å¯¶å¯¶ä½†é€¼è‡ªå·±å¤šåƒä¸€é»åˆé–‹å§‹å\n\n\nè€Œä¸”é‚„ä¸å¹¸æ„Ÿå†’ğŸ˜·é ­ç—›æ¬²è£‚\næœ‰å»å©¦ç”¢ç§‘çœ‹é†«ç”Ÿå¿«ä¸€å€‹æœˆé‚„å¥½ä¸äº†\nä¹Ÿä¸æ•¢ä¸€ç›´æ±‚é†«æ€•åƒå¤ªå¤šè—¥æœƒå½±éŸ¿å¯¶å¯¶\nå•é†«ç”Ÿæ€éº¼è¾¦ï¼Œä»–åªèªªé‚£å¦³è¦è¶•å¿«å¥½å•Šâ‹¯â‹¯\n\n\nå› ç‚ºé£Ÿæ…¾ä¸€ç›´ä¸æŒ¯\nä¸çŸ¥é“æœ‰æ²’æœ‰åª½åª½ï¼Œå¯ä»¥çµ¦æˆ‘ä¸€äº›å»ºè­°\næˆ‘æ˜¯ä¸æ˜¯è©²é–‹å§‹å–æ»´é›ç²¾æˆ–å…¶ä»–é£Ÿç‰©å‘¢ï¼Ÿ', 
                    # 'excerpt': 'æ‡·å­•è‡³ç›®å‰16é€±äº†ï¼Œæ‡·å­•ä¹‹å‰æ˜¯166/59ï¼Œçµæœå‰ä¸‰å€‹æœˆéå¸¸åš´é‡çš„å­•åè®“é«”é‡ç›´ç›´è½ç˜¦åˆ°51é‚„ä¼´éš¨è‘—äº›è¨±è½ç´…ï¼Œå¥½ä¸å®¹æ˜“ç†¬éä½†é£Ÿæ…¾é‚„æ˜¯ä¸è¦‹èµ·è‰²ï¼Œç›®å‰é£Ÿé‡å¤§ç´„5é¡†æ°´é¤ƒä¸€é¤ï¼Œå°±ç®—æ²’åƒä¹Ÿä¸è¦ºå¾—é¤“ï¼Œè¦ºå¾—å°ä¸èµ·å¯¶å¯¶ä½†é€¼è‡ªå·±å¤šåƒä¸€é»åˆé–‹å§‹å', 
                    # 'createdAt': '2018-12-20T10:10:14.846000Z', 
                    # 'updatedAt': '2018-12-20T10:10:14.846000', 
                    # 'commentCount': 19, 'likeCount': 23, 'forumName': 'çµå©š', 
                    # 'forumAlias': 'marriage', 'gender': 'F', 'school': 'æœé™½ç§‘æŠ€å¤§å­¸', 
                    # 'hidden': False, 'media': [], 'score': '2018-12-20T15:53:46.484000Z', 'order': '2018-12-20T10:10:14.846000Z'},
                    rjs = rj['data']['posts']
                    rx = rjs
                    #å°‡æ‰€éœ€è³‡æ–™çˆ¬å–å¾Œ å­˜é€²posts
                    for i in range(len(rx)):
                        posts = []
                        posts.append(rx[i]['order'])
                        posts.append(rx[i]['title'])
                        content = rx[i]['content']
                        content = ILLEGAL_CHARACTERS_RE.sub(r'', content)
                        posts.append(content)
                        posts.append(rx[i]['commentCount'])
                        posts.append(rx[i]['likeCount'])
                        posts.append(rx[i]['forumName'])
                        # title = ['ç™¼ä½ˆæ™‚é–“','æ–‡ç« æ¨™é¡Œ','å…§æ–‡','ç•™è¨€æ•¸','å–œæ­¡æ•¸','æ¨™ç±¤']
                        #å°‡è³‡æ–™å­˜é€²Excel
                        num +=1
                        ws.append(posts)
                        print(web + 'å·²å®Œæˆå¯«å…¥'+ str(num) + 'ç¯‡æ–‡ç« ')
                    # print(posts)
                except KeyError as k :
                    print(k)
                except AttributeError as attr:
                    print(attr)
                except NameError as name:
                    print(name)
                except IndexError as index:
                    print(index)
                except JSONDecodeError :
                    print(JSONDecodeError)

    #è¦åŠ saveæ‰èƒ½è¼¸å‡ºæˆExcel
    wb.save(keyword+ '_yet_' + web + '_ç¶²è·¯çˆ¬èŸ².xlsx')

    #è®€å–å‰›çˆ¬å–å¥½çš„æª”æ¡ˆ
    df = pd.read_excel(keyword+ '_yet_' + web + '_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´

    #æŸ¥çœ‹æ˜¯å¦æœ‰é‡è¤‡å€¼
    cd1 = df.duplicated('æ–‡ç« æ¨™é¡Œ').sum()
    print('é‡è¤‡æ–‡ç« æœ‰'+str(cd1)+'ç¯‡')

    #ä»¥æ–‡ç« æ¨™é¡Œä½œç‚ºæ¨™çš„ åˆªé™¤é‡è¤‡å€¼
    drop_data = df.drop_duplicates(subset=['æ–‡ç« æ¨™é¡Œ'])
    # print('ç¸½å…±æœ‰'+str(len(drop_data))+'ç¯‡æ–‡ç« ')
    #è¦åŠ to_excelæ‰èƒ½è¼¸å‡ºæˆExcel
    drop_data.to_excel(keyword+'_ok_'+'_'+ web +'_ç¶²è·¯çˆ¬èŸ².xlsx',index=False)

    df = pd.read_excel(keyword+ '_ok' +'_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´
    print(keyword+'_'+web+ '_ç¸½å…±æœ‰'+str(len(df))+'ç¯‡æ–‡ç« ')

    # PTTç¶²è·¯çˆ¬èŸ²
    # ä½¿ç”¨æ–¹å¼å¯æ›´æ”¹ 1.é—œéµå­— 2.å¯çˆ¬å¤šå€‹çœ‹æ¿(è‡ªè¡Œé¸æ“‡) 3.çˆ¬å–çš„é æ•¸ 
    # p.s. æ¨ å¹³ å™“ ç•™è¨€çš†å·²åˆ†é–‹ / è³‡æ–™æœ€çµ‚ä»¥excelå½¢å¼å­˜æ”¾
    from bs4 import BeautifulSoup
    import requests
    import json
    import csv
    from openpyxl import Workbook
    from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
    import re

    ILLEGAL_CHARACTERS_RE = re.compile(r'[\000-\010]|[\013-\014]|[\016-\037]')

    wb = Workbook()
    ws = wb.active

    title =["ä½œè€…",'çœ‹æ¿',"æ–‡ç« æ¨™é¡Œ","ç™¼ä½ˆæ—¥æœŸ","æ–‡ç« å…§å®¹","æ¨æ¨ç•™è¨€",'å¹³å¹³ç•™è¨€','å™“å™“ç•™è¨€']

    ws.append(title)

    web = 'PTT'
    # board_names = ['Gossiping', 'Stock', 'Baseball', 'C_Chat', 'LoL', 'NBA', 'Lifeismoney', 'Elephants', 'HatePolitics', 'Military', 'car', 'Tech_Job', 'Beauty', 'home-sale', 'KoreaStar', 'Lions', 'BabyMother', 'DIABLO', 'MobileComm', 'Boy-Girl', 'movie', 'sex', 'Monkeys', 'WomenTalk', 'PC_Shopping', 'marriage', 'Badminton', 'KoreaDrama', 'AllTogether', 'DigiCurrency', 'Kaohsiung', 'Tainan', 'Guardians', 'PlayStation', 'basketballTW', 'TaichungBun', 'japanavgirls', 'KR_Entertain', 'creditcard', 'joke', 'NSwitch', 'Steam', 'TaiwanDrama', 'China-Drama', 'CVS', 'CFantasy', 'Marginalman', 'marvel', 'e-shopping', 'Japandrama', 'SportLottery', 'Insurance', 'iOS', 'EAseries', 'HardwareSale', 'forsale', 'BeautySalon', 'Hsinchu', 'AC_In', 'KoreanPop', 'Gamesale', 'Soft_Job', 'StupidClown', 'biker', 'MacShop', 'watch', 'TW_Entertain', 'PuzzleDragon', 'YuanChuang', 'Headphone', 'Salary', 'studyteacher', 'Option', 'FORMULA1', 'PublicServan', 'CarShop', 'Tennis', 'ToS', 'MakeUp', 'FATE_GO', 'fastfood', 'PokemonGO', 'MuscleBeach', 'BaseballXXXX', 'Brand', 'DMM_GAMES', 'NBA_Film', 'ONE_PIECE', 'E-appliance', 'MLB', 'Taoyuan', 'Bank_Service', 'nb-shopping', 'WOW', 'nCoV2019', 'YUGIOH', 'cookclub', 'Hearthstone', 'SakaTalk', 'Gov_owned', 'miHoYo', 'mobilesales', 'cat', 'Wanted', 'BabyProducts', 'gay', 'give', 'Teacher', 'Examination', 'GetMarry', 'SuperBike', 'Finance', 'hypermall', 'Palmar_Drama', 'Food', 'job', 'Arknights', 'Key_Mou_Pad', 'Digitalhome', 'UmaMusume', 'MH', 'Vtuber', 'FTV', 'Railway', 'Hip-Hop', 'HelpBuy', 'feminine_sex', 'Coffee']
    board_names = ['Gossiping', 'Beauty','BabyMother','Boy-Girl',
        'WomenTalk','sex','marriage','AllTogether','KoreaDrama',
        'TaichungBun','joke','Marginalman','e-shopping','BeautySalon'
        'AC_In','Gamesale','MakeUp','fastfood','nCoV2019','BabyProducts',
        'cookclub','Food','Key_Mou_Pad']
    # # åŸºæœ¬åƒæ•¸
    # # url = "https://www.ptt.cc/bbs/"+board_names+"/search?q="+ keyword
    # # for j in range(2):
    payload = {
        'from': '/bbs/Gossiping/index.html',
        'yes': 'yes'
    }
    data = []   # å…¨éƒ¨æ–‡ç« çš„è³‡æ–™
    num = 0 #

    # # ç”¨sessionç´€éŒ„æ­¤æ¬¡ä½¿ç”¨çš„cookieï¼Œå°±æ˜¯æŒ‰éä¸€æ¬¡18ï¼Œç¬¬äºŒæ¬¡å°±ä¸å¿…å†æŒ‰
    rs = requests.session()
    response = rs.post("https://www.ptt.cc/ask/over18", data=payload)
    page_urls = []
    num = 0
    for board_name in board_names:
        #range å¯ä»¥èª¿æ•´çˆ¬èŸ²çš„é æ•¸
        for i in range(1,11):
            url = "https://www.ptt.cc/bbs/"+board_name+'/search?page='+str(i)+'&q='+keyword
        # çˆ¬å–å…©é 
                # getå–å¾—é é¢çš„HTML
                # print(url)
            response = rs.get(url)
            soup = BeautifulSoup(response.text, "html.parser")
            # print(soup.prettify()) 

            # æ‰¾å‡ºæ¯ç¯‡æ–‡ç« çš„é€£çµ
            links = soup.find_all("div", class_="title")
            for link in links:
                # å¦‚æœæ–‡ç« å·²è¢«åˆªé™¤ï¼Œé€£çµç‚ºNone
                if link.a != None:
                    article_data = {}   # å–®ç¯‡æ–‡ç« çš„è³‡æ–™
                    page_url = "https://www.ptt.cc/"+link.a["href"]
                    if page_url not in page_urls :
                        page_urls.append(page_url)

    # print(page_urls)
    print(web + 'ç¸½å…±æœ‰' +str(len(page_urls))+'å€‹ç¶²å€')
    # page_urls =['https://www.ptt.cc//bbs/Gossiping/M.1648429698.A.8CE.html', 'https://www.ptt.cc//bbs/Gossiping/M.1644230304.A.FD3.html', 'https://www.ptt.cc//bbs/Gossiping/M.1643785151.A.374.html', 'https://www.ptt.cc//bbs/Gossiping/M.1642565644.A.5FF.html', 'https://www.ptt.cc//bbs/Gossiping/M.1640247990.A.53C.html', 'https://www.ptt.cc//bbs/Gossiping/M.1639223413.A.BA6.html', 'https://www.ptt.cc//bbs/Gossiping/M.1639222570.A.5DC.html', 'https://www.ptt.cc//bbs/Gossiping/M.1639222163.A.CC0.html', 'https://www.ptt.cc//bbs/Gossiping/M.1638419405.A.A28.html', 'https://www.ptt.cc//bbs/Gossiping/M.1635037526.A.519.html', 'https://www.ptt.cc//bbs/Gossiping/M.1633216902.A.ED8.html', 'https://www.ptt.cc//bbs/Gossiping/M.1631795899.A.ABC.html', 'https://www.ptt.cc//bbs/Gossiping/M.1631780466.A.21B.html', 'https://www.ptt.cc//bbs/Gossiping/M.1630490246.A.AE5.html', 'https://www.ptt.cc//bbs/Gossiping/M.1630073547.A.292.html', 'https://www.ptt.cc//bbs/Gossiping/M.1629223525.A.12F.html', 'https://www.ptt.cc//bbs/Gossiping/M.1626313436.A.AE0.html', 'https://www.ptt.cc//bbs/Gossiping/M.1624774427.A.257.html', 'https://www.ptt.cc//bbs/Gossiping/M.1623123641.A.3AD.html', 'https://www.ptt.cc//bbs/Gossiping/M.1622704613.A.BD5.html', 'https://www.ptt.cc//bbs/Gossiping/M.1621039775.A.F12.html', 'https://www.ptt.cc//bbs/Gossiping/M.1618761431.A.F32.html', 'https://www.ptt.cc//bbs/Gossiping/M.1618618396.A.AAF.html']

    try :
        article_data = {}   # å–®ç¯‡æ–‡ç« çš„è³‡æ–™

        # é€²å…¥æ–‡ç« é é¢
        for n in page_urls :
            response = rs.get(n)
            result = BeautifulSoup(response.text, "html.parser")
            # print(result)
            # break
            # print(soup.prettify()) #ç¢ºèªå–®å€‹é é¢æ¨™é¡Œé‚£äº›

            # æ‰¾å‡ºä½œè€…ã€æ¨™é¡Œã€æ™‚é–“ã€ç•™è¨€
            main_content = result.find("div", id="main-content")
            article_info = main_content.find_all("span", class_="article-meta-value")
            author = article_info[0].string  # ä½œè€…
            board = article_info[1].string #çœ‹æ¿
            title = article_info[2].string  # æ¨™é¡Œ
            time = article_info[3].string   # æ™‚é–“
            # if len(article_info) != 0: #ä¸ç­‰æ–¼å°±è·³å‡º(è·‘ä¸€æ¬¡å°±è·³å®Œ)
            #     author = article_info[0].string  # ä½œè€…
            #     title = article_info[2].string  # æ¨™é¡Œ
            #     time = article_info[3].string   # æ™‚é–“
            # else:
            #     author = "ç„¡"  # ä½œè€…
            #     title = "ç„¡"  # æ¨™é¡Œ
            #     time = "ç„¡"   # æ™‚é–“

            # print(author)
            # print(title)
            # print(time)
            # break
                # break

            article_data["author"] = author
            article_data['board'] = board
            article_data["title"] = title
            article_data["time"] = time #åˆ°æ™‚å€™è¦å–å‡ºå®¹å™¨è³‡æ–™-ä½œè€…æ–‡ç« æ™‚é–“
            # print(article_data)
            # å°‡æ•´æ®µæ–‡å­—å…§å®¹æŠ“å‡ºä¾†ï¼Œå› ç‚ºæ²’æœ‰divå®¹å™¨åŒ…è‘—
            all_text = main_content.text
            # ä»¥--åˆ‡å‰²ï¼ŒæŠ“æœ€å¾Œä¸€å€‹--å‰çš„æ‰€æœ‰å…§å®¹
            pre_texts = all_text.split("--")[:-1]
            # å°‡å‰é¢çš„æ‰€æœ‰å…§å®¹åˆä½µæˆä¸€å€‹
            one_text = "--".join(pre_texts)
            # ä»¥\nåˆ‡å‰²ï¼Œç¬¬ä¸€è¡Œæ¨™é¡Œä¸è¦
            texts = one_text.split("\n")[1:]
            # å°‡æ¯ä¸€è¡Œåˆä½µ
            content = "\n".join(texts)
            # print(content)

            article_data["content"] = content  #åˆ°æ™‚å€™è¦å–å‡ºå®¹å™¨è³‡æ–™-å…§å®¹
            #     #ç•™è¨€è·Ÿæ›é çš„code
            #     # ä¸€ç¨®ç•™è¨€ä¸€å€‹åˆ—è¡¨ï¼Œé€™é‚Šæ˜¯ä¾†é¡¯ç¤ºã€æ¨ã€å¹³ã€å™“
            comment_dic = {}
            push_dic = []
            arrow_dic = []
            shu_dic = []

            # æŠ“å‡ºæ‰€æœ‰ç•™è¨€
            comments = main_content.find_all("div", class_="push")
            #BeautifulSoupä¸èƒ½ä½¿ç”¨.textå–æ–‡å­—ï¼Œéœ€è¦ä½¿ç”¨åƒæ•¸è½‰æ›æˆå­—ä¸²
            for comment in comments:
                push_tag = comment.find(
                    "span", class_="push-tag").string   # åˆ†é¡æ¨™ç±¤
                push_userid = comment.find(
                    "span", class_="push-userid").string  # ä½¿ç”¨è€…ID
                push_content = comment.find(
                    "span", class_="push-content").string   # ç•™è¨€å…§å®¹
                push_time = comment.find(
                    "span", class_="push-ipdatetime").string   # ç•™è¨€æ™‚é–“

                # print(push_tag, push_userid, push_content, push_time)

                # dict1 = {'push_tag':push_tag,"push_userid": push_userid,
                #         "push_content" : push_content, "push_time": push_time}

                dict1 = {"push_userid": push_userid,
                        "push_content" : push_content, "push_time": push_time}
                if push_tag == "æ¨ " :
                    push_dic.append(dict1)
                elif push_tag == "â†’ ":
                    arrow_dic.append(dict1)
                else:
                    shu_dic.append(dict1)
                # else:
                #     break
            # print(dict1)
            #         #è¾¨è­˜å­—ä¸²å…§å®¹ï¼Œè½‰æˆæ–‡å­—åœ¨é™„åŠ åˆ°å­—å…¸è£¡é¢
                # print(dict1["push_content"])
            # print(type(dict1))

            # # # # #é€™é‚Šè¼¸å‡ºæœƒæœ‰ä¸€å †ä»£ç¢¼ï¼Œç„¶å¾Œé‚„è¦é‚„éœ€è¦è¼¸å‡ºçµ¦é‚£3å€‹è¡¨

            # print(push_dic)
            # print(arrow_dic)
            # print(shu_dic)
            # print("--------")

            comment_dic["æ¨"] = push_dic
            comment_dic["â†’"] = arrow_dic
            comment_dic["å™“"] = shu_dic
            article_data["comment"] = comment_dic
            #comment_dict å­˜æ”¾è‘—å·²åˆ†å¥½çš„æ¨ å¹³ å™“ ä¸‰ç¨®è©•è«–å…§å®¹
            # print(comment_dic)

            # print(article_data) #çœ‹ä¸€ä¸‹æœªæ•´ç†çš„ï¼Œä½†å­˜åœ¨é€™å€‹å®¹å™¨å…§çš„è³‡æ–™
            #{'author': 'id520 (ä¹Ÿç„¡é¢¨é›¨ä¹Ÿç„¡æ™´)', 
            # 'title': '[å•å¦] è¶…å•†æœ‰æ²’æœ‰ç”šéº¼ç›Šç”ŸèŒæˆ–å„ªæ ¼ç”¢å“å¯ä»¥æ¨', 
            # 'time': 'Mon Mar 28 09:08:15 2022', 
            # 'content': '\n  å‰›å¥½æœ‰è¶…å•†ä¸€ç™¾å…ƒæŠµç”¨åˆ¸ æƒ³èªªä¾†é¡§å€‹å¥åº·\n\n  æœ‰æ²’æœ‰ç”šéº¼ç›Šç”ŸèŒæˆ–æ˜¯å„ªæ ¼ç”¢å“å€¼å¾—è²·çš„\n\n  ä»Šå¤©å°è‚¡åˆå¤§è·Œäº†\n\n  ä¸Šç¦®æ‹œäº”è³ éŒ¢è³£èˆˆå¯Œç™¼è²·é€²å°ç©é›»\n\n  çµæœä»Šå¤©åˆå¥—ç‰¢äº†  æˆ‘æ˜¯ä¸æ˜¯çœŸçš„è¢«ä¸»åŠ›ç›¯ä¸Šäº†\n\n\n', 
            # 'comment': {'æ¨': [{'push_tag': 'æ¨ ', 'push_userid': 'lpbrother', 'push_content': ': é¤Šæ¨‚å¤š', 'push_time': '101.136.131.159 03/28 09:14\n'}, {'push_tag': 'æ¨ ', 'push_userid': 'likebadday', 'push_content': ': ç¦æ¨‚é»ƒè‰²åŒ…è£å°é®®å¥¶å„ªé…ªï¼Œå£æ„Ÿæ‰å¯¦ç¶¿', 'push_time': '  1.160.196.224 03/30 02:47\n'}], 'â†’': [{'push_tag': 'â†’ ', 'push_userid': 'ansfan', 'push_content': ': è‘¡çœ¾', 'push_time': '  49.216.130.81 03/28 09:08\n'}, {'push_tag': 'â†’ ', 'push_userid': 'likebadday', 'push_content': ': å¯†ã€è…¸èƒƒè •å‹•é †', 'push_time': '  1.160.196.224 03/30 02:47\n'}], 'å™“': []}}

            data.append(article_data)#å°‡æ‰€æœ‰æ–‡ç« å…§å®¹å¡é€²"data"è£¡é¢

            for m in data:
                author = m['author'] 
                board = m['board']
                title = m['title']
                time = m['time']
                content =m['content']
                num += 1
                comment_push = []
                for a in range(len(m['comment']['æ¨'])):
                    push = str(m['comment']['æ¨'][a]['push_content'])
                    comment_push.append(push)
                comment_push=str(comment_push)

                comment_flat = []
                for b in range(len(m['comment']['â†’'])):
                    flat = str(m['comment']['â†’'][b]['push_content'])
                    comment_flat.append(flat)
                comment_flat=str(comment_flat)

                comment_shu = []
                for c in range(len(m['comment']['å™“'])):
                    shu = str(m['comment']['å™“'][c]['push_content'])
                    comment_shu.append(shu)
                comment_shu=str(comment_shu)

                items = [author,board,title,time,content,comment_push,comment_flat,comment_shu]
                
                excel_contents = []
                for w in items:
                    w= ILLEGAL_CHARACTERS_RE.sub(r'', w)
                    excel_contents.append(w)
                
                # print(excel_contents)
                print(web + 'å·²å®Œæˆå¯«å…¥' + str(num) + 'ç¯‡æ–‡ç« ')
                ws.append(excel_contents)
                break
                # print(excel_contents)
                # print(excel_contents)
                # print(author1,title,time,content,comment_push,comment_flat,comment_shu,sep='===')
    except KeyError:
        print(KeyError)          
    except AttributeError as attr:
        print(attr)
    except NameError as name:
        print(name)
    except IndexError as index:
        print(index)
    except JSONDecodeError :
        print(JSONDecodeError)
    wb.save(keyword + '_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx')

    df = pd.read_excel(keyword + '_'+ web +'_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´
    print(keyword+'_'+web+ '_ç¸½å…±æœ‰'+str(len(df))+'ç¯‡æ–‡ç« ')


    from bs4 import BeautifulSoup
    from openpyxl import Workbook
    import parsel #æ•¸æ“šè§£ææ¨¡çµ„
    import requests #æ•¸æ“šè«‹æ±‚æ¨¡çµ„
    import csv
    import json
    from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
    import re

    ILLEGAL_CHARACTERS_RE = re.compile(r'[\000-\010]|[\013-\014]|[\016-\037]')

    wb = Workbook()
    ws = wb.active

    title =["æ–‡ç« æ¨™é¡Œ","æ–‡ç« åˆ†é¡","ç™¼ä½ˆæ—¥æœŸ","æ–‡ç« å…§å®¹","æ–‡ç« ç¶²å€"]

    ws.append(title)

    web = 'TVBS'

    #ç‚ºé˜²çˆ¬èŸ²ç›£æ§ æ‰€ä»¥æ”¹æˆè‡ªå·±çš„user-agent

    # header = {
    #     'æ”¹æˆè‡ªå·±çš„ç¶²å€'
    # }
    article_url=[]

    for i in range(1,11):
        url = 'https://news.tvbs.com.tw/news/searchresult/'+keyword+'/news/'+str(i)
        response = requests.get(url=url , headers = headers)
        selector = parsel.Selector(response.text) # æŠŠç²å–ä¸‹ä¾†çš„htmlå­—ç¬¦ä¸²æ•¸æ“šï¼Œè½‰æˆselectorå¯è§£æçš„å°è±¡
        # print (selector) #<Selector xpath=None data='<html lang="zh-CN" class="ua-mac ua-w...'>
        # cssé¸æ“‡å™¨ï¼šå°±æ˜¯æ ¹æ“šæ¨™ç±¤å±¬æ€§å…§å®¹ï¼Œæå–ç›¸é—œæ•¸æ“š
        lis = selector.css('.news_list .list ul li') #ç¬¬ä¸€æ¬¡æå–ï¼Œç²å–æ‰€æœ‰liæ¨™ç±¤ è¿”å›åˆ—è¡¨
        # print(lis)
        for li in lis :
            href = li.css('a::attr(href)').get() #è©³æƒ…é  ç²å–attrå±¬æ€§
            article_url.append(href)

    print(web +'ç¸½å…±æœ‰'+ str(len(article_url)) +'å€‹ç¶²å€') 
    # #çˆ¬å–æ‰€æœ‰æ–‡ç« çš„ç¶²å€


    #çˆ¬å–æ‰€æœ‰æ–‡ç« çš„å…§æ–‡
    num = 0
    for url in article_url:
        try:
        # print(url)
        # 1.ç™¼é€è«‹æ±‚

            res = requests.get(url=url,headers=headers)

            soup = BeautifulSoup(res.text,"lxml")
            json_file = soup.select("script[type='application/ld+json']")
            # print(type(json_file))
            #<class 'bs4.element.ResultSet'>
            for i in json_file:
                targets = json.loads(i.get_text(strip=True),strict=False)
                #strict=False ä¸è¦é‚£éº¼åš´è¬¹ å­—å…¸å–ä¸åˆ°çš„ å°±ç®—äº†
                titles_list=["headline","articleSection","datePublished","articleBody","mainEntityOfPage"]
                #title =["æ–‡ç« æ¨™é¡Œ","æ–‡ç« åˆ†é¡","ç™¼ä½ˆæ—¥æœŸ","æ–‡ç« å…§å®¹","æ–‡ç« ç¶²å€"]
                article_info = []

                for title in titles_list:                
                    y = targets[title]
                    y= ILLEGAL_CHARACTERS_RE.sub(r'', y)
                    article_info.append(y)

                    # print(title,':',y)            
                num+=1
                ws.append(article_info)
                print(web + 'å·²å®Œæˆå¯«å…¥' + str(num) + 'ç¯‡æ–‡ç« ')
                break
        except KeyError as k :
            print(k)       
        except AttributeError as attr:
            print(attr)
        except NameError as name:
            print(name)
        except IndexError as index:
            print(index)
        except JSONDecodeError :
            print(JSONDecodeError)

    wb.save(keyword + '_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx')
    import pandas as pd
    df = pd.read_excel(keyword + '_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´
    print(keyword+'_'+web+ '_ç¸½å…±æœ‰'+str(len(df))+'ç¯‡æ–‡ç« ')

    #è¯äººå¥åº·ç¶²çˆ¬èŸ²
    from bs4 import BeautifulSoup
    # from urllib.request#ç”¨æ–¼ç²å–ç¶²é 
    import requests
    import json
    from openpyxl import Workbook
    import parsel
    from openpyxl.cell.cell import ILLEGAL_CHARACTERS_RE
    import re

    ILLEGAL_CHARACTERS_RE = re.compile(r'[\000-\010]|[\013-\014]|[\016-\037]')

    wb = Workbook()
    ws = wb.active

    title =["æ–‡ç« æ¨™é¡Œ",'æ–‡ç« å…§å®¹','ç™¼ä½ˆæ—¥æœŸ']

    ws.append(title)

    web = 'è¯äººå¥åº·ç¶²'
    # åŸºæœ¬åƒæ•¸
    article_url = []
    num_url = 0
    for num in range(1,11):
        try:
            # url = "https://www.top1health.com/Search?q="+keyword+"&page="+str(num)+"&1&type=Illness"
            url = 'https://www.top1health.com/Search?q='+ keyword +'&page='+str(num)+'&type=All'
            rs = requests.session()

            # print(url)
            # getå–å¾—é é¢çš„HTML
            response = requests.get(url, headers=headers) 
            response.encoding = 'utf8'
            soup = BeautifulSoup(response.text, "html.parser")
            # print(soup.text) #å–å¾—é é¢è³‡æ–™
            links = soup.find_all(class_="post-title") #center-block ul li a
            # print(links)
            for link in links:
                if link.a != None:
                    page_url = "https://www.top1health.com/"+link.a["href"]
                    #------é€™è£¡æœ‰ä¸€å€‹cookiesé˜²ç«ç‰†----
                    response_page = requests.get(page_url, headers=headers)
                    result = BeautifulSoup(response_page.text, "html.parser")
                    # print(rponse_pagees)  #ç¢ºèªæ‹¿åˆ°çš„ç¶²å€ï¼Œç­‰ç­‰è¦é€²å…¥æ–‡ç«  200
                    # article_data["articleURL"] = page_url
                    article_url.append(page_url)
                    num_url+=1
                    print(web + 'å·²å®Œæˆå¯«å…¥' + str(num_url) + 'å€‹ç¶²å€')
        except KeyError as k :
            print(k)        
        except AttributeError as attr:
            print(attr)
        except NameError as name:
            print(name)
        except IndexError as index:
            print(index)
        except JSONDecodeError :
            print(JSONDecodeError)

    #å–å¾—æ‰€æœ‰æ–‡ç« çš„ç¶²å€
    # print(article_url)
    print('ç¸½å…±ç²å–äº†',len(article_url),'å€‹ç¶²å€')
    # print(article_url)

    num_article=0
    for url in article_url:
        try:

            # url = "https://www.top1health.com/Search?q="+KEY+"&page="+str(num)+"&1&type=Illness"
            rs = requests.session()

            response = requests.get(url, headers=headers) 
            response.encoding = 'utf8'
            soup = BeautifulSoup(response.text, "html.parser")
            # print(soup.text) #å–å¾—é é¢è³‡æ–™
            article_info =[]
            
            headline = soup.find("h2") #center-block ul li a
            article_headline = headline.text
            article_info.append(article_headline)
            
            contents = soup.find_all("p") #center-block ul li a
            article_content=''
            for content in contents:
                article_content += content.text
            
            article_content= ILLEGAL_CHARACTERS_RE.sub(r'', article_content)
            article_info.append(article_content)

            selector = parsel.Selector(response.text) # æŠŠç²å–ä¸‹ä¾†çš„htmlå­—ç¬¦ä¸²æ•¸æ“šï¼Œè½‰æˆselectorå¯è§£æçš„å°è±¡
            # print (selector) #<Selector xpath=None data='<html lang="zh-CN" class="ua-mac ua-w...'>
            # cssé¸æ“‡å™¨ï¼šå°±æ˜¯æ ¹æ“šæ¨™ç±¤å±¬æ€§å…§å®¹ï¼Œæå–ç›¸é—œæ•¸æ“š
            # .å¾Œé¢+classåç¨± æ²’æœ‰.ä»£è¡¨æ˜¯æ¨™ç±¤åç¨± :nth-child(i) è¡¨ç¤ºå–é€™å€‹æ¨™ç±¤çš„ç¬¬iå€‹
            # åªæœ‰ä¸€ç­†è³‡æ–™ç”¨get() æ¨™ç±¤å…§å«æœ‰å¤šç¨®æˆ‘å€‘è¦çš„è³‡æ–™ç”¨getall()
            info = selector.css('.info') #ç¬¬ä¸€æ¬¡æå–ï¼Œç²å–æ‰€æœ‰liæ¨™ç±¤ è¿”å›åˆ—è¡¨
            published_time = info.css('span:nth-child(2)::text').get()
            # print(title)
            article_info.append(published_time)
            # print(article_info)
            ws.append(article_info)
            num_article+=1
            print(web + 'å·²å®Œæˆå¯«å…¥' + str(num_article) + 'ç¯‡æ–‡ç« ')
            # break
        except KeyError as k :
            print(k)
        except AttributeError as attr:
            print(attr)
        except NameError as name:
            print(name)
        except IndexError as index:
            print(index)
        except JSONDecodeError :
            print(JSONDecodeError)

    wb.save(keyword + '_yet'+'_'+ web + '_ç¶²è·¯çˆ¬èŸ².xlsx')

    import pandas as pd
    df = pd.read_excel(keyword + '_yet'+'_'+ web +'_ç¶²è·¯çˆ¬èŸ².xlsx') #è«‹è‡ªè¡Œä¾æª”æ¡ˆä½ç½®èª¿æ•´
    # print(keyword+'_'+web+ '_ç¸½å…±æœ‰'+str(len(df))+'ç¯‡æ–‡ç« ')

    cd1 = df.duplicated('æ–‡ç« å…§å®¹').sum()
    print(web +'é‡è¤‡æ–‡ç« æœ‰'+str(cd1))

    # #ä»¥æ–‡ç« æ¨™é¡Œä½œç‚ºæ¨™çš„ åˆªé™¤é‡è¤‡å€¼
    drop_data = df.drop_duplicates(subset=['æ–‡ç« å…§å®¹'])

    drop_data.to_excel(keyword+'_ok'+'_'+ web +'_ç¶²è·¯çˆ¬èŸ².xlsx',index=False)
    print(web + 'ç¸½å…±æ–‡ç« æœ‰'+str(len(drop_data)))



    import requests # è¨ªå•
    import jieba
    from collections import Counter # æ¬¡æ•¸çµ±è¨ˆ
    import re
    from tkinter import font
    import matplotlib.pyplot as plt
    from wordcloud import WordCloud
    import numpy
    from PIL import Image # åœ–ç‰‡è½‰arrayé™£åˆ—
    import pandas as pd
    import csv
    import random
    from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator  
    from matplotlib import colors
    # titles = ['ç™¼ä½ˆæ—¥æœŸ','æ–‡ç« æ¨™é¡Œ','æ–‡ç« å…§å®¹',"ç•™è¨€å…§å®¹",'é—œéµå­—',
    #         "æ–‡ç« ID","ä½œè€…",'çœ‹æ¿',"æ¨æ¨ç•™è¨€",'å¹³å¹³ç•™è¨€','å™“å™“ç•™è¨€',
    #         "æ–‡ç« åˆ†é¡","æ–‡ç« ç¶²å€",'ç•™è¨€æ•¸','å–œæ­¡æ•¸','æ¨™ç±¤']

    web = 'total'
    target_content = 'æ–‡ç« æ¨™é¡Œ'
    target_content2 = 'æ–‡ç« å…§å®¹'
    target_content3 = 'ç•™è¨€å…§å®¹'

    all_article = keyword +'_ç¶²è·¯çˆ¬èŸ².txt'
    filename1 = keyword+'_åª½å’ªæ‹œ_ç¶²è·¯çˆ¬èŸ².xlsx'
    title =["æ–‡ç« æ¨™é¡Œ","æ–‡ç« å…§å®¹"]
    df = pd.read_excel(filename1)#,sheetname ='Sheet',header=None)
    df1=df[target_content]
    df2=df[target_content2]
    # df3=df[target_content3]
    df_a=pd.concat([df1,df2])
    a = len(df1)
    # df4.to_csv(a,header=None,sep=',',index=False)

    filename2 = keyword+'_åª½å’ªæ„›_ç¶²è·¯çˆ¬èŸ².xlsx'
    title = ["æ–‡ç« æ¨™é¡Œ","æ–‡ç« å…§å®¹","ç•™è¨€å…§å®¹"]
    df = pd.read_excel(filename2)#,sheetname ='Sheet',header=None)
    df1=df[target_content]
    df2=df[target_content2]
    df3=df[target_content3]
    df_b=pd.concat([df1,df2,df3])
    b = len(df1)
    # df4.to_csv(a,header=None,sep=',',index=False)

    filename3 =keyword+'_ok_Dcardæ·±å¡_ç¶²è·¯çˆ¬èŸ².xlsx'
    title = ['æ–‡ç« æ¨™é¡Œ','æ–‡ç« å…§å®¹']
    df = pd.read_excel(filename3)#,sheetname ='Sheet',header=None)
    df1=df[target_content]
    df2=df[target_content2]
    # df3=df[target_content3]
    df_c=pd.concat([df1,df2])
    c = len(df1)

    # df4.to_csv(a,header=None,sep=',',index=False)

    filename4 =  keyword+'_PTT_ç¶²è·¯çˆ¬èŸ².xlsx' 
    title =["æ–‡ç« æ¨™é¡Œ","æ–‡ç« å…§å®¹","æ¨æ¨ç•™è¨€",'å¹³å¹³ç•™è¨€','å™“å™“ç•™è¨€']
    df = pd.read_excel(filename4)#,sheetname ='Sheet',header=None)
    df1=df[target_content]
    df2=df[target_content2]
    df3_1=df["æ¨æ¨ç•™è¨€"]
    df3_2=df["å¹³å¹³ç•™è¨€"]
    df3_3=df["å™“å™“ç•™è¨€"]
    # df3=df[target_content3]
    df_d=pd.concat([df1,df2,df3_1,df3_2,df3_3])
    d = len(df1)

    # df4.to_csv(a,header=None,sep=',',index=False)

    filename5 = keyword+'_TVBS_ç¶²è·¯çˆ¬èŸ².xlsx'
    title =["æ–‡ç« æ¨™é¡Œ","æ–‡ç« å…§å®¹"]
    df = pd.read_excel(filename5)#,sheetname ='Sheet',header=None)
    df1=df[target_content]
    df2=df[target_content2]
    # df3=df[target_content3]
    df_e=pd.concat([df1,df2])
    e = len(df1)

    # df4.to_csv(a,header=None,sep=',',index=False)

    filename6 =  keyword +'_ok_è¯äººå¥åº·ç¶²_ç¶²è·¯çˆ¬èŸ².xlsx'
    title =["æ–‡ç« æ¨™é¡Œ",'æ–‡ç« å…§å®¹']
    df = pd.read_excel(filename6)#,sheetname ='Sheet',header=None)
    df1=df[target_content]
    df2=df[target_content2]
    # df3=df[target_content3]
    df_f=pd.concat([df1,df2])
    f = len(df1)


    df_all = pd.concat([df_a,df_b,df_c,df_d,df_e,df_f])
    df_all.to_csv(all_article,header=None,sep=',',index=False)

    print('åª½å’ªæ‹œç¸½å…±æœ‰',a,'ç¯‡æ–‡ç« ')
    print('åª½å’ªæ„›ç¸½å…±æœ‰',b,'ç¯‡æ–‡ç« ')
    print('Dcardæ·±å¡ç¸½å…±æœ‰',c,'ç¯‡æ–‡ç« ')
    print('PTTç¸½å…±æœ‰',d,'ç¯‡æ–‡ç« ')
    print('TVBSç¸½å…±æœ‰',e,'ç¯‡æ–‡ç« ')
    print('è¯äººå¥åº·ç¶²ç¸½å…±æœ‰',f,'ç¯‡æ–‡ç« ')
    print('===================')
    print(keyword + 'ç¸½å…±æœ‰'+str(a+b+c+d+e+f)+'ç¯‡æ–‡ç« ')

    print('å·²å®ŒæˆåŒ¯å‡º')

    # with open(all_article, encoding="utf-8", errors='ignore') as f:
    #     text = f.read()
    with open(all_article, encoding="utf-8", errors='ignore') as f:
        text = f.read()

# crawler_all(keyword=keyword)
